# IBM â€“ Unsupervised Machine Learning: Full Course Notes

## **MODULE 1: Introduction to Unsupervised Learning and K-Means**

### **1. Tá»•ng quan ná»™i dung**
Module Ä‘áº§u tiÃªn giá»›i thiá»‡u vá» **há»c khÃ´ng giÃ¡m sÃ¡t (Unsupervised Learning)** â€“ má»™t phÆ°Æ¡ng phÃ¡p trong Machine Learning nháº±m **khÃ¡m phÃ¡ cáº¥u trÃºc tiá»m áº©n trong dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n**. Thay vÃ¬ huáº¥n luyá»‡n tá»« cÃ¡c cáº·p (x, y) nhÆ° trong Supervised Learning, Unsupervised Learning tÃ¬m kiáº¿m **má»‘i quan há»‡ giá»¯a cÃ¡c máº«u dá»¯ liá»‡u**.

Thuáº­t toÃ¡n ná»•i báº­t trong pháº§n nÃ y lÃ  **K-Means Clustering**, má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n cá»¥m (clustering) dá»±a trÃªn khoáº£ng cÃ¡ch.

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Unsupervised Learning (Há»c khÃ´ng giÃ¡m sÃ¡t):** PhÆ°Æ¡ng phÃ¡p há»c tá»« dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n, má»¥c tiÃªu lÃ  khÃ¡m phÃ¡ cáº¥u trÃºc tiá»m áº©n.
- **Cluster (Cá»¥m):** NhÃ³m cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao.
- **Centroid (TÃ¢m cá»¥m):** Trung tÃ¢m cá»§a má»™t cá»¥m, tÃ­nh báº±ng trung bÃ¬nh cá»™ng cÃ¡c Ä‘iá»ƒm trong cá»¥m.
- **K-Means Algorithm:** Thuáº­t toÃ¡n phÃ¢n cá»¥m phá»• biáº¿n, láº·p láº¡i viá»‡c gÃ¡n Ä‘iá»ƒm dá»¯ liá»‡u vÃ o cá»¥m gáº§n nháº¥t vÃ  cáº­p nháº­t tÃ¢m cá»¥m.
- **Iteration (Láº·p):** QuÃ¡ trÃ¬nh cáº­p nháº­t liÃªn tá»¥c tÃ¢m cá»¥m cho Ä‘áº¿n khi há»™i tá»¥.

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**
Má»¥c tiÃªu cá»§a K-Means lÃ  **tá»‘i thiá»ƒu hoÃ¡ tá»•ng bÃ¬nh phÆ°Æ¡ng khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  tÃ¢m cá»¥m tÆ°Æ¡ng á»©ng**:

$$J = \sum_{i=1}^{k}\sum_{x \in C_i} \lVert x - \mu_i \rVert^2$$

Trong Ä‘Ã³:
- **J:** hÃ m má»¥c tiÃªu (objective function) â€“ thá»ƒ hiá»‡n Ä‘á»™ sai lá»‡ch tá»•ng.
- **k:** sá»‘ cá»¥m (clusters).
- **x:** Ä‘iá»ƒm dá»¯ liá»‡u.
- **Î¼áµ¢:** tÃ¢m cá»¥m thá»© i (centroid of cluster i).
- **â€–x - Î¼áµ¢â€–Â²:** bÃ¬nh phÆ°Æ¡ng khoáº£ng cÃ¡ch **Euclidean** giá»¯a Ä‘iá»ƒm x vÃ  tÃ¢m cá»¥m.

**Khoáº£ng cÃ¡ch Euclidean:**

$$d(x, \mu_i) = \sqrt{(x_1 - \mu_{i1})^2 + (x_2 - \mu_{i2})^2 + \dots + (x_n - \mu_{in})^2}$$

Khoáº£ng cÃ¡ch nÃ y Ä‘o Ä‘á»™ khÃ¡c nhau giá»¯a hai vector trong khÃ´ng gian n-chiá»u.

---

### **4. VÃ­ dá»¥ minh hoáº¡**
Giáº£ sá»­ ta cÃ³ 6 Ä‘iá»ƒm dá»¯ liá»‡u trÃªn máº·t pháº³ng 2D, cáº§n chia thÃ nh **k = 2** cá»¥m:

1. Chá»n ngáº«u nhiÃªn 2 tÃ¢m cá»¥m ban Ä‘áº§u.
2. GÃ¡n má»—i Ä‘iá»ƒm vÃ o cá»¥m cÃ³ tÃ¢m gáº§n nháº¥t.
3. Cáº­p nháº­t tÃ¢m cá»¥m báº±ng trung bÃ¬nh cá»§a cÃ¡c Ä‘iá»ƒm trong cá»¥m.
4. Láº·p láº¡i Ä‘áº¿n khi tÃ¢m cá»¥m khÃ´ng thay Ä‘á»•i nhiá»u.

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
- Äiá»ƒm: A(1,1), B(2,1), C(4,3), D(5,4), E(1,2), F(5,3)
- Khá»Ÿi táº¡o: Î¼â‚ = (1,1), Î¼â‚‚ = (5,4)
- Iteration 1: GÃ¡n A,B,E vÃ o Câ‚; C,D,F vÃ o Câ‚‚
- Cáº­p nháº­t: Î¼â‚_new = (1.33, 1.33), Î¼â‚‚_new = (4.67, 3.33)
- Láº·p láº¡i Ä‘áº¿n khi há»™i tá»¥

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**
- Cáº§n chá»n trÆ°á»›c sá»‘ cá»¥m k (thÆ°á»ng xÃ¡c Ä‘á»‹nh báº±ng **Elbow Method**).
- Dá»¯ liá»‡u nÃªn Ä‘Æ°á»£c **chuáº©n hoÃ¡ (Standardization)** Ä‘á»ƒ trÃ¡nh bias do khÃ¡c thang Ä‘o.
- Káº¿t quáº£ phá»¥ thuá»™c vÃ o **khá»Ÿi táº¡o tÃ¢m cá»¥m** (initialization); dÃ¹ng **K-Means++** Ä‘á»ƒ tá»‘i Æ°u hoÃ¡ chá»n tÃ¢m ban Ä‘áº§u.
- K-Means hoáº¡t Ä‘á»™ng tá»‘t vá»›i cá»¥m hÃ¬nh cáº§u, kÃ©m hiá»‡u quáº£ vá»›i cá»¥m hÃ¬nh dáº¡ng phá»©c táº¡p.

---

### **6. Key Takeaways**
 Unsupervised Learning khÃ´ng cáº§n nhÃ£n, tÃ¬m cáº¥u trÃºc áº©n trong dá»¯ liá»‡u  
 K-Means tá»‘i Æ°u hÃ³a khoáº£ng cÃ¡ch Ä‘iá»ƒm Ä‘áº¿n tÃ¢m cá»¥m  
 Cáº§n chuáº©n hÃ³a dá»¯ liá»‡u vÃ  chá»n k há»£p lÃ½ (Elbow Method)  
 K-Means++ giÃºp khá»Ÿi táº¡o tá»‘t hÆ¡n, trÃ¡nh local minima  
 PhÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ cá»¥m hÃ¬nh cáº§u vÃ  phÃ¢n bá»‘ Ä‘á»u  

---

## **MODULE 2: Distance Metrics & Computational Hurdles**

### **1. Tá»•ng quan ná»™i dung**
Module nÃ y táº­p trung vÃ o **cÃ¡c Ä‘á»™ Ä‘o khoáº£ng cÃ¡ch (Distance Metrics)** â€“ cÃ´ng cá»¥ cá»‘t lÃµi Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u trong clustering. Viá»‡c chá»n metric phÃ¹ há»£p áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n cháº¥t lÆ°á»£ng phÃ¢n cá»¥m.

NgoÃ i ra, module Ä‘á» cáº­p Ä‘áº¿n **cÃ¡c thÃ¡ch thá»©c tÃ­nh toÃ¡n** khi xá»­ lÃ½ dá»¯ liá»‡u lá»›n vÃ  cÃ¡ch giáº£i quyáº¿t.

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Euclidean Distance (Khoáº£ng cÃ¡ch Euclid):** Khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng giá»¯a hai Ä‘iá»ƒm trong khÃ´ng gian.
- **Manhattan Distance (Khoáº£ng cÃ¡ch Manhattan):** Tá»•ng Ä‘á»™ chÃªnh lá»‡ch theo tá»«ng trá»¥c tá»a Ä‘á»™ (nhÆ° Ä‘i trÃªn lÆ°á»›i Ã´ vuÃ´ng).
- **Minkowski Distance:** Tá»•ng quÃ¡t hÃ³a cá»§a Euclidean vÃ  Manhattan vá»›i tham sá»‘ p.
- **Cosine Similarity (Äá»™ tÆ°Æ¡ng tá»± Cosine):** Äo gÃ³c giá»¯a hai vector, phÃ¹ há»£p vá»›i dá»¯ liá»‡u vÄƒn báº£n.
- **Normalization/Standardization:** Chuáº©n hÃ³a dá»¯ liá»‡u vá» cÃ¹ng thang Ä‘o trÆ°á»›c khi tÃ­nh khoáº£ng cÃ¡ch.
- **Computational Complexity (Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n):** Chi phÃ­ tÃ­nh toÃ¡n tÄƒng theo kÃ­ch thÆ°á»›c dá»¯ liá»‡u.

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**

#### **3.1. Euclidean Distance**
$$d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

Äo khoáº£ng cÃ¡ch Ä‘Æ°á»ng tháº³ng trong khÃ´ng gian n-chiá»u. Nháº¡y cáº£m vá»›i outliers vÃ  khÃ¡c biá»‡t vá» thang Ä‘o.

#### **3.2. Manhattan Distance**
$$d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

Tá»•ng giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a chÃªnh lá»‡ch trÃªn tá»«ng chiá»u. Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers hÆ¡n Euclidean.

#### **3.3. Minkowski Distance**
$$d(x, y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$$

- **p = 1:** Manhattan Distance
- **p = 2:** Euclidean Distance
- **p â†’ âˆ:** Chebyshev Distance (max|xáµ¢ - yáµ¢|)

#### **3.4. Cosine Similarity**
$$\text{cosine}(x,y) = \frac{x \cdot y}{\lVert x \rVert \lVert y \rVert} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}$$

**Cosine Distance:**
$$d_{\text{cosine}}(x,y) = 1 - \text{cosine}(x,y)$$

Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá» hÆ°á»›ng, khÃ´ng quan tÃ¢m Ä‘á»™ lá»›n. PhÃ¹ há»£p vá»›i text mining, recommendation systems.

---

### **4. VÃ­ dá»¥ minh hoáº¡**

Cho hai Ä‘iá»ƒm: **x = (1, 2)**, **y = (4, 6)**

#### **Euclidean:**
$$d = \sqrt{(1-4)^2 + (2-6)^2} = \sqrt{9+16} = \sqrt{25} = 5$$

#### **Manhattan:**
$$d = |1-4| + |2-6| = 3 + 4 = 7$$

#### **Cosine Similarity:**
$$\text{cosine} = \frac{1Ã—4 + 2Ã—6}{\sqrt{1^2+2^2} \cdot \sqrt{4^2+6^2}} = \frac{16}{\sqrt{5} \cdot \sqrt{52}} = \frac{16}{16.12} \approx 0.993$$

$$d_{\text{cosine}} = 1 - 0.993 = 0.007$$

**Nháº­n xÃ©t:** Cosine distance ráº¥t nhá» â†’ hai vector gáº§n nhÆ° cÃ¹ng hÆ°á»›ng.

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**

#### **5.1. Computational Hurdles (ThÃ¡ch thá»©c tÃ­nh toÃ¡n)**
- Vá»›i n Ä‘iá»ƒm dá»¯ liá»‡u, tÃ­nh táº¥t cáº£ khoáº£ng cÃ¡ch cáº·p: **O(nÂ²)** â€“ khÃ´ng kháº£ thi vá»›i dá»¯ liá»‡u lá»›n.
- K-Means chuáº©n: má»—i iteration tÃ­nh khoáº£ng cÃ¡ch nÃ—k láº§n â†’ **O(nÃ—kÃ—tÃ—d)** vá»›i t iterations, d dimensions.

#### **5.2. Giáº£i phÃ¡p tá»‘i Æ°u**
- **MiniBatch K-Means:** Chá»‰ dÃ¹ng má»™t pháº§n dá»¯ liá»‡u má»—i iteration.
- **Approximate Nearest Neighbor (ANN):** DÃ¹ng cáº¥u trÃºc dá»¯ liá»‡u nhÆ° KD-Tree, Ball Tree.
- **Dimensionality Reduction:** DÃ¹ng PCA giáº£m sá»‘ chiá»u trÆ°á»›c khi clustering.
- **Parallel Computing:** Táº­n dá»¥ng GPU, distributed computing.

#### **5.3. Lá»±a chá»n metric phÃ¹ há»£p**
- **Euclidean:** Dá»¯ liá»‡u sá»‘ liá»‡u, cá»¥m hÃ¬nh cáº§u, Ä‘Ã£ chuáº©n hÃ³a.
- **Manhattan:** Dá»¯ liá»‡u cÃ³ outliers, grid-based structures.
- **Cosine:** Text data, sparse vectors, quan tÃ¢m hÆ°á»›ng hÆ¡n Ä‘á»™ lá»›n.

---

### **6. Key Takeaways**
 Euclidean phÃ¹ há»£p vá»›i dá»¯ liá»‡u sá»‘ Ä‘Ã£ chuáº©n hÃ³a, cá»¥m compact  
 Manhattan Ã­t nháº¡y vá»›i outliers, tá»‘t cho dá»¯ liá»‡u grid  
 Cosine Ä‘o Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng hÆ°á»›ng, lÃ½ tÆ°á»Ÿng cho text/sparse data  
 LuÃ´n chuáº©n hÃ³a dá»¯ liá»‡u trÆ°á»›c khi tÃ­nh khoáº£ng cÃ¡ch  
 Vá»›i dá»¯ liá»‡u lá»›n: dÃ¹ng MiniBatch K-Means, ANN, hoáº·c PCA  

---

## **MODULE 3: Selecting a Clustering Algorithm**

### **1. Tá»•ng quan ná»™i dung**
Module nÃ y hÆ°á»›ng dáº«n cÃ¡ch **lá»±a chá»n thuáº­t toÃ¡n phÃ¢n cá»¥m phÃ¹ há»£p** dá»±a trÃªn Ä‘áº·c Ä‘iá»ƒm dá»¯ liá»‡u: kÃ­ch thÆ°á»›c, hÃ¬nh dáº¡ng cá»¥m, nhiá»…u, vÃ  má»¥c tiÃªu phÃ¢n tÃ­ch. Má»—i thuáº­t toÃ¡n cÃ³ Æ°u nhÆ°á»£c Ä‘iá»ƒm riÃªng, khÃ´ng cÃ³ "best algorithm for all".

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Hierarchical Clustering (PhÃ¢n cá»¥m phÃ¢n cáº¥p):** XÃ¢y dá»±ng cÃ¢y phÃ¢n cáº¥p (dendrogram) thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c cá»¥m.
- **DBSCAN (Density-Based Spatial Clustering):** PhÃ¢n cá»¥m dá»±a trÃªn máº­t Ä‘á»™ Ä‘iá»ƒm.
- **Gaussian Mixture Model (GMM):** MÃ´ hÃ¬nh há»—n há»£p Gaussian, gÃ¡n xÃ¡c suáº¥t cho má»—i Ä‘iá»ƒm thuá»™c cá»¥m.
- **Dendrogram:** Biá»ƒu Ä‘á»“ cÃ¢y thá»ƒ hiá»‡n quÃ¡ trÃ¬nh gá»™p/chia cá»¥m trong hierarchical clustering.
- **Soft Clustering:** Cho phÃ©p má»™t Ä‘iá»ƒm thuá»™c nhiá»u cá»¥m vá»›i xÃ¡c suáº¥t khÃ¡c nhau (GMM).
- **Hard Clustering:** Má»—i Ä‘iá»ƒm chá»‰ thuá»™c Ä‘Ãºng 1 cá»¥m (K-Means, DBSCAN).

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**

#### **3.1. Hierarchical Clustering**

**Agglomerative (Bottom-up):**
1. Má»—i Ä‘iá»ƒm lÃ  má»™t cá»¥m
2. Gá»™p 2 cá»¥m gáº§n nháº¥t
3. Láº·p láº¡i Ä‘áº¿n khi cÃ²n 1 cá»¥m

**Linkage criteria:**
- **Single Linkage:** min d(a,b) vá»›i a âˆˆ Câ‚, b âˆˆ Câ‚‚
- **Complete Linkage:** max d(a,b)
- **Average Linkage:** trung bÃ¬nh táº¥t cáº£ d(a,b)
- **Ward's Method:** tá»‘i thiá»ƒu variance khi gá»™p cá»¥m

#### **3.2. Gaussian Mixture Model (GMM)**

$$P(x) = \sum_{i=1}^{k} \pi_i \cdot \mathcal{N}(x | \mu_i, \Sigma_i)$$

Trong Ä‘Ã³:
- **Ï€áµ¢:** Trá»ng sá»‘ cá»¥m i (mixing coefficient), âˆ‘Ï€áµ¢ = 1
- **ğ’©(x|Î¼áµ¢, Î£áµ¢):** PhÃ¢n phá»‘i Gaussian vá»›i mean Î¼áµ¢ vÃ  covariance Î£áµ¢
- **Î¼áµ¢:** Vector trung bÃ¬nh cá»§a cá»¥m i
- **Î£áµ¢:** Ma tráº­n hiá»‡p phÆ°Æ¡ng sai cá»§a cá»¥m i

**PhÃ¢n phá»‘i Gaussian Ä‘a chiá»u:**

$$\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$

**Thuáº­t toÃ¡n EM (Expectation-Maximization):**
- **E-step:** TÃ­nh xÃ¡c suáº¥t Ä‘iá»ƒm x thuá»™c cá»¥m i
- **M-step:** Cáº­p nháº­t Ï€áµ¢, Î¼áµ¢, Î£áµ¢

---

### **4. VÃ­ dá»¥ minh hoáº¡**

#### **So sÃ¡nh 4 thuáº­t toÃ¡n trÃªn cÃ¹ng dataset:**

**Dataset:** Dá»¯ liá»‡u 2D vá»›i 3 cá»¥m khÃ´ng Ä‘á»u, cÃ³ outliers.

| Thuáº­t toÃ¡n | Káº¿t quáº£ |
|-----------|---------|
| **K-Means** | Táº¡o 3 cá»¥m hÃ¬nh cáº§u, outliers bá»‹ gÃ¡n sai |
| **Hierarchical** | Dendrogram cho tháº¥y cáº¥u trÃºc phÃ¢n cáº¥p, nhÆ°ng outliers váº«n trong cá»¥m |
| **DBSCAN** | Nháº­n diá»‡n Ä‘Ãºng 3 cá»¥m + Ä‘Ã¡nh dáº¥u outliers lÃ  noise |
| **GMM** | Táº¡o 3 cá»¥m vá»›i xÃ¡c suáº¥t, má»™t sá»‘ Ä‘iá»ƒm biÃªn cÃ³ xÃ¡c suáº¥t trung gian |

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**

#### **5.1. Báº£ng so sÃ¡nh chi tiáº¿t**

| TiÃªu chÃ­ | K-Means | Hierarchical | DBSCAN | GMM |
|---------|---------|--------------|--------|-----|
| **Sá»‘ cá»¥m** | Pháº£i chá»n trÆ°á»›c | Cáº¯t dendrogram | Tá»± Ä‘á»™ng | Pháº£i chá»n trÆ°á»›c |
| **HÃ¬nh dáº¡ng cá»¥m** | HÃ¬nh cáº§u | Linh hoáº¡t | Báº¥t ká»³ | Ellipsoid |
| **Outliers** | Nháº¡y cáº£m | Nháº¡y cáº£m | Xá»­ lÃ½ tá»‘t | Trung bÃ¬nh |
| **Complexity** | O(nkt) | O(nÂ²logn) | O(nlogn) | O(nkÂ²t) |
| **Soft/Hard** | Hard | Hard | Hard | Soft |
| **Scale** | Tá»‘t (n lá»›n) | KÃ©m (n nhá») | Trung bÃ¬nh | Trung bÃ¬nh |

#### **5.2. Decision Tree Ä‘á»ƒ chá»n thuáº­t toÃ¡n**

```
Dá»¯ liá»‡u cÃ³ nhiá»…u/outliers?
â”œâ”€ YES â†’ DBSCAN
â””â”€ NO â†’ Biáº¿t trÆ°á»›c sá»‘ cá»¥m k?
    â”œâ”€ YES â†’ Cá»¥m hÃ¬nh cáº§u?
    â”‚   â”œâ”€ YES â†’ K-Means
    â”‚   â””â”€ NO â†’ GMM
    â””â”€ NO â†’ Hierarchical
```

#### **5.3. LÆ°u Ã½ khi Ã¡p dá»¥ng**
- **K-Means:** Cháº¡y nhiá»u láº§n vá»›i khá»Ÿi táº¡o khÃ¡c nhau, dÃ¹ng K-Means++.
- **Hierarchical:** Chá»n linkage method phÃ¹ há»£p (Ward thÆ°á»ng tá»‘t nháº¥t).
- **DBSCAN:** Tuning epsilon (Îµ) vÃ  MinPts quan trá»ng, dÃ¹ng k-distance graph.
- **GMM:** Dá»… overfit, cáº§n regularization hoáº·c BIC/AIC Ä‘á»ƒ chá»n k.

---

### **6. Key Takeaways**
 KhÃ´ng cÃ³ thuáº­t toÃ¡n tá»‘t nháº¥t cho má»i trÆ°á»ng há»£p  
 K-Means: nhanh, Ä‘Æ¡n giáº£n, nhÆ°ng giáº£ Ä‘á»‹nh cá»¥m hÃ¬nh cáº§u  
 Hierarchical: khÃ´ng cáº§n chá»n k, nhÆ°ng tá»‘n bá»™ nhá»›  
 DBSCAN: xá»­ lÃ½ outliers tá»‘t, tÃ¬m cá»¥m hÃ¬nh dáº¡ng báº¥t ká»³  
 GMM: soft clustering, linh hoáº¡t nhÆ°ng giáº£ Ä‘á»‹nh Gaussian  
 Chá»n thuáº­t toÃ¡n dá»±a trÃªn: hÃ¬nh dáº¡ng cá»¥m, outliers, quy mÃ´ dá»¯ liá»‡u  

---

## **MODULE 4: Clustering Evaluation Metrics**

### **1. Tá»•ng quan ná»™i dung**
Sau khi phÃ¢n cá»¥m, cáº§n **Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng** káº¿t quáº£. Module nÃ y giá»›i thiá»‡u cÃ¡c metrics Ä‘á»ƒ:
- Äo Ä‘á»™ compact (cháº·t cháº½) trong cá»¥m
- Äo Ä‘á»™ separation (tÃ¡ch biá»‡t) giá»¯a cÃ¡c cá»¥m
- So sÃ¡nh cÃ¡c thuáº­t toÃ¡n/tham sá»‘ khÃ¡c nhau

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Inertia/Within-Cluster Sum of Squares (WCSS):** Tá»•ng bÃ¬nh phÆ°Æ¡ng khoáº£ng cÃ¡ch trong cá»¥m.
- **Silhouette Score:** Äo Ä‘á»™ tÃ¡ch biá»‡t giá»¯a cÃ¡c cá»¥m, tá»« -1 Ä‘áº¿n 1.
- **Davies-Bouldin Index (DBI):** Äo tá»· lá»‡ giá»¯a phÃ¢n tÃ¡n trong cá»¥m vÃ  khoáº£ng cÃ¡ch giá»¯a cÃ¡c cá»¥m.
- **Calinski-Harabasz Index:** Tá»· lá»‡ giá»¯a phÃ¢n tÃ¡n giá»¯a cÃ¡c cá»¥m vÃ  trong cá»¥m.
- **Elbow Method:** PhÆ°Æ¡ng phÃ¡p chá»n k báº±ng cÃ¡ch tÃ¬m Ä‘iá»ƒm gáº¥p khÃºc trÃªn Ä‘á»“ thá»‹ Inertia.

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**

#### **3.1. Inertia (WCSS)**
$$J = \sum_{i=1}^{k}\sum_{x \in C_i} \lVert x - \mu_i \rVert^2$$

- CÃ ng nhá» cÃ ng tá»‘t (cá»¥m cÃ ng compact)
- Giáº£m khi k tÄƒng â†’ khÃ´ng dÃ¹ng riÃªng Ä‘á»ƒ chá»n k
- DÃ¹ng Elbow Method Ä‘á»ƒ tÃ¬m k tá»‘i Æ°u

#### **3.2. Silhouette Score**

**Cho tá»«ng Ä‘iá»ƒm x:**
$$s(x) = \frac{b(x) - a(x)}{\max(a(x), b(x))}$$

Trong Ä‘Ã³:
- **a(x):** Khoáº£ng cÃ¡ch trung bÃ¬nh tá»« x Ä‘áº¿n cÃ¡c Ä‘iá»ƒm khÃ¡c trong cÃ¹ng cá»¥m (cohesion)
- **b(x):** Khoáº£ng cÃ¡ch trung bÃ¬nh tá»« x Ä‘áº¿n cÃ¡c Ä‘iá»ƒm trong cá»¥m gáº§n nháº¥t khÃ¡c (separation)

**Silhouette Score tá»•ng thá»ƒ:**
$$S = \frac{1}{n}\sum_{i=1}^{n} s(x_i)$$

**Diá»…n giáº£i:**
- **s(x) â‰ˆ 1:** Äiá»ƒm Ä‘Æ°á»£c phÃ¢n cá»¥m tá»‘t
- **s(x) â‰ˆ 0:** Äiá»ƒm náº±m giá»¯a 2 cá»¥m
- **s(x) < 0:** Äiá»ƒm cÃ³ thá»ƒ bá»‹ phÃ¢n cá»¥m sai

#### **3.3. Davies-Bouldin Index (DBI)**
$$DBI = \frac{1}{k}\sum_{i=1}^{k} \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$$

Trong Ä‘Ã³:
- **Ïƒáµ¢:** Äá»™ phÃ¢n tÃ¡n trung bÃ¬nh trong cá»¥m i
- **d(cáµ¢, câ±¼):** Khoáº£ng cÃ¡ch giá»¯a tÃ¢m cá»¥m i vÃ  j

**Diá»…n giáº£i:** CÃ ng nhá» cÃ ng tá»‘t (cá»¥m compact vÃ  tÃ¡ch biá»‡t).

#### **3.4. Calinski-Harabasz Index (CH)**
$$CH = \frac{SS_B/(k-1)}{SS_W/(n-k)}$$

Trong Ä‘Ã³:
- **SS_B:** Between-cluster sum of squares
- **SS_W:** Within-cluster sum of squares
- **n:** Sá»‘ Ä‘iá»ƒm, **k:** Sá»‘ cá»¥m

**Diá»…n giáº£i:** CÃ ng lá»›n cÃ ng tá»‘t (phÃ¢n tÃ¡n giá»¯a cá»¥m lá»›n, trong cá»¥m nhá»).

---

### **4. VÃ­ dá»¥ minh hoáº¡**

#### **4.1. Elbow Method**

Cháº¡y K-Means vá»›i k = 1, 2, 3, ..., 10, tÃ­nh Inertia:

| k | Inertia |
|---|---------|
| 1 | 3500 |
| 2 | 1800 |
| 3 | 950 |
| 4 | 850 |
| 5 | 820 |

**Äá»“ thá»‹:** Gáº¥p khÃºc rÃµ táº¡i k=3 â†’ chá»n k=3.

#### **4.2. Silhouette Score**

Dataset vá»›i 100 Ä‘iá»ƒm, cháº¡y K-Means k=3:

- Cá»¥m 1: 30 Ä‘iá»ƒm, S_avg = 0.75
- Cá»¥m 2: 45 Ä‘iá»ƒm, S_avg = 0.82
- Cá»¥m 3: 25 Ä‘iá»ƒm, S_avg = 0.68

**Tá»•ng:** S = (30Ã—0.75 + 45Ã—0.82 + 25Ã—0.68)/100 = 0.77 â†’ Tá»‘t!

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**

#### **5.1. So sÃ¡nh cÃ¡c metrics**

| Metric | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | GiÃ¡ trá»‹ tá»‘t |
|--------|---------|------------|-------------|
| **Inertia** | ÄÆ¡n giáº£n, nhanh | KhÃ´ng dÃ¹ng riÃªng Ä‘á»ƒ chá»n k | CÃ ng nhá» |
| **Silhouette** | Trá»±c quan, [-1,1] | Cháº­m vá»›i n lá»›n | Gáº§n 1 |
| **DBI** | XÃ©t cáº£ cohesion & separation | Nháº¡y vá»›i outliers | CÃ ng nhá» |
| **CH** | Hiá»‡u quáº£ vá»›i n lá»›n | Giáº£ Ä‘á»‹nh cá»¥m convex | CÃ ng lá»›n |

#### **5.2. Quy trÃ¬nh Ä‘Ã¡nh giÃ¡**

```python
# Pseudo-code
for k in range(2, 11):
    model = KMeans(n_clusters=k)
    labels = model.fit_predict(X)
    
    inertia[k] = model.inertia_
    silhouette[k] = silhouette_score(X, labels)
    dbi[k] = davies_bouldin_score(X, labels)
    ch[k] = calinski_harabasz_score(X, labels)

# Váº½ Ä‘á»“ thá»‹ vÃ  chá»n k tá»‘i Æ°u
```

#### **5.3. LÆ°u Ã½ quan trá»ng**
- **Elbow Method:** ÄÃ´i khi khÃ´ng cÃ³ Ä‘iá»ƒm gáº¥p khÃºc rÃµ rÃ ng.
- **Silhouette:** TÃ­nh toÃ¡n O(nÂ²) â†’ cháº­m vá»›i dá»¯ liá»‡u lá»›n.
- **Káº¿t há»£p nhiá»u metrics:** KhÃ´ng nÃªn chá»‰ dá»±a vÃ o 1 chá»‰ sá»‘.
- **Ground truth:** Náº¿u cÃ³ nhÃ£n tháº­t, dÃ¹ng Adjusted Rand Index (ARI), Normalized Mutual Information (NMI).

---

### **6. Key Takeaways**
 Inertia giáº£m khi k tÄƒng, dÃ¹ng Elbow Method Ä‘á»ƒ chá»n k  
 Silhouette Score [-1,1]: gáº§n 1 lÃ  tá»‘t, <0 lÃ  phÃ¢n cá»¥m sai  
 DBI vÃ  CH Ä‘Ã¡nh giÃ¡ cáº£ cohesion vÃ  separation  
 NÃªn káº¿t há»£p nhiá»u metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n  
 Vá»›i ground truth: dÃ¹ng ARI, NMI thay vÃ¬ internal metrics  

---

## **MODULE 5: Density-Based Clustering and DBSCAN**

### **1. Tá»•ng quan ná»™i dung**
**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** lÃ  thuáº­t toÃ¡n phÃ¢n cá»¥m dá»±a trÃªn **máº­t Ä‘á»™ Ä‘iá»ƒm** thay vÃ¬ khoáº£ng cÃ¡ch Ä‘áº¿n tÃ¢m cá»¥m. Æ¯u Ä‘iá»ƒm lá»›n: **nháº­n diá»‡n cá»¥m hÃ¬nh dáº¡ng báº¥t ká»³** vÃ  **tá»± Ä‘á»™ng phÃ¡t hiá»‡n outliers**.

KhÃ´ng cáº§n chá»n sá»‘ cá»¥m k trÆ°á»›c, phÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ nhiá»…u.

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Density (Máº­t Ä‘á»™):** Sá»‘ Ä‘iá»ƒm trong má»™t vÃ¹ng lÃ¢n cáº­n.
- **Îµ (epsilon):** BÃ¡n kÃ­nh vÃ¹ng lÃ¢n cáº­n.
- **MinPts:** Sá»‘ Ä‘iá»ƒm tá»‘i thiá»ƒu trong vÃ¹ng Îµ Ä‘á»ƒ táº¡o cá»¥m.
- **Core Point (Äiá»ƒm lÃµi):** Äiá»ƒm cÃ³ â‰¥ MinPts Ä‘iá»ƒm trong vÃ¹ng Îµ.
- **Border Point (Äiá»ƒm biÃªn):** Náº±m trong vÃ¹ng Îµ cá»§a core point nhÆ°ng khÃ´ng Ä‘á»§ MinPts.
- **Noise Point (Äiá»ƒm nhiá»…u):** KhÃ´ng thuá»™c cá»¥m nÃ o.
- **Directly Density-Reachable:** Äiá»ƒm q trong vÃ¹ng Îµ cá»§a core point p.
- **Density-Reachable:** CÃ³ chuá»—i core points ná»‘i tá»« p Ä‘áº¿n q.

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**

#### **3.1. Äá»‹nh nghÄ©a lÃ¢n cáº­n Îµ**

$N_{\varepsilon}(p) = \{q \in D \mid d(p,q) \leq \varepsilon\}$

Táº­p há»£p cÃ¡c Ä‘iá»ƒm q cÃ³ khoáº£ng cÃ¡ch Ä‘áº¿n p khÃ´ng quÃ¡ Îµ.

#### **3.2. Core Point Condition**

Äiá»ƒm p lÃ  core point náº¿u:
$|N_{\varepsilon}(p)| \geq MinPts$

#### **3.3. Directly Density-Reachable**

Äiá»ƒm q directly density-reachable tá»« p náº¿u:
- p lÃ  core point
- q âˆˆ N_Îµ(p)

#### **3.4. Density-Reachable**

q density-reachable tá»« p náº¿u tá»“n táº¡i chuá»—i: p = pâ‚, pâ‚‚, ..., pâ‚™ = q

Sao cho páµ¢â‚Šâ‚ directly density-reachable tá»« páµ¢.

#### **3.5. Density-Connected**

p vÃ  q density-connected náº¿u tá»“n táº¡i Ä‘iá»ƒm o sao cho cáº£ p vÃ  q Ä‘á»u density-reachable tá»« o.

**Má»™t cá»¥m trong DBSCAN** lÃ  táº­p táº¥t cáº£ cÃ¡c Ä‘iá»ƒm density-connected vá»›i nhau.

---

### **4. VÃ­ dá»¥ minh hoáº¡**

#### **4.1. Thuáº­t toÃ¡n DBSCAN tá»«ng bÆ°á»›c**

**Dataset:** 12 Ä‘iá»ƒm, Îµ = 1.5, MinPts = 3

**BÆ°á»›c 1:** Chá»n Ä‘iá»ƒm chÆ°a thÄƒm (A)
- TÃ¬m N_Îµ(A) = {A, B, C, D} â†’ |N_Îµ(A)| = 4 â‰¥ 3 â†’ A lÃ  core point
- Táº¡o Cluster 1 = {A, B, C, D}

**BÆ°á»›c 2:** Má»Ÿ rá»™ng tá»« B (trong Cluster 1)
- N_Îµ(B) = {A, B, E} â†’ B lÃ  core point
- ThÃªm E vÃ o Cluster 1

**BÆ°á»›c 3:** Tiáº¿p tá»¥c vá»›i C, D, E...

**BÆ°á»›c 4:** Äiá»ƒm X khÃ´ng Ä‘áº¿n Ä‘Æ°á»£c tá»« core point nÃ o â†’ X lÃ  noise

**Káº¿t quáº£:**
- Cluster 1: {A, B, C, D, E, F}
- Cluster 2: {G, H, I}
- Noise: {X, Y}

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**

#### **5.1. Chá»n tham sá»‘ Îµ vÃ  MinPts**

**MinPts:**
- ThÆ°á»ng chá»n: MinPts = 2Ã—dim (dim lÃ  sá»‘ chiá»u)
- Tá»‘i thiá»ƒu: MinPts = 3 (cho dá»¯ liá»‡u 2D)
- Dá»¯ liá»‡u nhiá»u chiá»u hoáº·c nhiá»…u: tÄƒng MinPts

**Îµ (epsilon):**
- DÃ¹ng **k-distance graph:**
  1. TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n Ä‘iá»ƒm thá»© k gáº§n nháº¥t (k = MinPts)
  2. Sáº¯p xáº¿p tÄƒng dáº§n
  3. Váº½ Ä‘á»“ thá»‹, tÃ¬m Ä‘iá»ƒm "gáº¥p khÃºc" â†’ chá»n Îµ

```python
# Pseudo-code k-distance
from sklearn.neighbors import NearestNeighbors
nbrs = NearestNeighbors(n_neighbors=MinPts).fit(X)
distances, indices = nbrs.kneighbors(X)
distances = np.sort(distances[:, -1])
plt.plot(distances)  # TÃ¬m elbow
```

#### **5.2. Æ¯u vÃ  NhÆ°á»£c Ä‘iá»ƒm**

**Æ¯u Ä‘iá»ƒm:**
 Nháº­n diá»‡n cá»¥m hÃ¬nh dáº¡ng báº¥t ká»³ (khÃ´ng chá»‰ hÃ¬nh cáº§u)
 Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  loáº¡i bá» outliers
 KhÃ´ng cáº§n chá»n sá»‘ cá»¥m k trÆ°á»›c
 Robust vá»›i nhiá»…u

**NhÆ°á»£c Ä‘iá»ƒm:**
 Nháº¡y cáº£m vá»›i tham sá»‘ Îµ vÃ  MinPts
 KhÃ´ng hiá»‡u quáº£ náº¿u máº­t Ä‘á»™ cá»¥m thay Ä‘á»•i nhiá»u
 KhÃ³ xá»­ lÃ½ dá»¯ liá»‡u cao chiá»u (curse of dimensionality)
 Äá»™ phá»©c táº¡p O(nÂ²) â†’ cháº­m vá»›i dá»¯ liá»‡u lá»›n (cáº£i thiá»‡n báº±ng spatial index)

#### **5.3. Biáº¿n thá»ƒ cá»§a DBSCAN**

- **HDBSCAN:** Hierarchical DBSCAN, tá»± Ä‘á»™ng chá»n Îµ cho tá»«ng vÃ¹ng
- **OPTICS:** Ordering Points To Identify Clustering Structure, khÃ´ng cáº§n chá»n Îµ cá»‘ Ä‘á»‹nh
- **DBSCAN++:** Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t vá»›i spatial indexing

---

### **6. Key Takeaways**
 DBSCAN phÃ¢n cá»¥m dá»±a máº­t Ä‘á»™, tÃ¬m cá»¥m hÃ¬nh dáº¡ng báº¥t ká»³  
 Tá»± Ä‘á»™ng phÃ¡t hiá»‡n outliers (noise points)  
 KhÃ´ng cáº§n chá»n sá»‘ cá»¥m k trÆ°á»›c  
 Tham sá»‘ quan trá»ng: Îµ (bÃ¡n kÃ­nh) vÃ  MinPts (ngÆ°á»¡ng máº­t Ä‘á»™)  
 DÃ¹ng k-distance graph Ä‘á»ƒ chá»n Îµ há»£p lÃ½  
 Háº¡n cháº¿: nháº¡y vá»›i tham sá»‘, kÃ©m hiá»‡u quáº£ náº¿u máº­t Ä‘á»™ khÃ´ng Ä‘á»u  

---

## **MODULE 6: Dimensionality Reduction Techniques (PCA & t-SNE)**

### **1. Tá»•ng quan ná»™i dung**
**Giáº£m chiá»u dá»¯ liá»‡u (Dimensionality Reduction)** lÃ  quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i dá»¯ liá»‡u tá»« khÃ´ng gian nhiá»u chiá»u sang khÃ´ng gian Ã­t chiá»u hÆ¡n, trong khi váº«n **giá»¯ láº¡i thÃ´ng tin quan trá»ng**. Má»¥c tiÃªu:
- Visualization (dá»¯ liá»‡u 2D/3D)
- Giáº£m chi phÃ­ tÃ­nh toÃ¡n
- Loáº¡i bá» nhiá»…u vÃ  multicollinearity
- Tiá»n xá»­ lÃ½ trÆ°á»›c clustering

Hai ká»¹ thuáº­t chÃ­nh: **PCA (linear)** vÃ  **t-SNE (non-linear)**.

---

### **2. Thuáº­t ngá»¯ chÃ­nh (Key Terms)**
- **Principal Component Analysis (PCA):** TÃ¬m cÃ¡c trá»¥c chÃ­nh (principal components) giá»¯ láº¡i phÆ°Æ¡ng sai lá»›n nháº¥t.
- **Eigenvalue (Trá»‹ riÃªng):** Äo lÆ°á»£ng phÆ°Æ¡ng sai giáº£i thÃ­ch bá»Ÿi má»—i eigenvector.
- **Eigenvector (Vector riÃªng):** HÆ°á»›ng cá»§a principal component.
- **Covariance Matrix (Ma tráº­n hiá»‡p phÆ°Æ¡ng sai):** Ma tráº­n Ä‘o má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a cÃ¡c features.
- **Explained Variance Ratio:** Tá»· lá»‡ phÆ°Æ¡ng sai Ä‘Æ°á»£c giá»¯ láº¡i bá»Ÿi tá»«ng PC.
- **t-SNE (t-Distributed Stochastic Neighbor Embedding):** Ká»¹ thuáº­t giáº£m chiá»u phi tuyáº¿n, giá»¯ cáº¥u trÃºc cá»¥c bá»™.
- **Perplexity:** Tham sá»‘ t-SNE, cÃ¢n báº±ng giá»¯a cáº¥u trÃºc local vÃ  global.

---

### **3. CÃ´ng thá»©c toÃ¡n há»c & Giáº£i thÃ­ch**

### **3.1. PCA (Principal Component Analysis)**

#### **BÆ°á»›c 1: Chuáº©n hÃ³a dá»¯ liá»‡u**
$X_{std} = \frac{X - \mu}{\sigma}$

#### **BÆ°á»›c 2: TÃ­nh ma tráº­n hiá»‡p phÆ°Æ¡ng sai**
$\Sigma = \frac{1}{n-1}X^T X$

Vá»›i X Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a (mean = 0).

#### **BÆ°á»›c 3: TÃ¬m eigenvalues vÃ  eigenvectors**

Giáº£i phÆ°Æ¡ng trÃ¬nh:
$\Sigma v = \lambda v$

Trong Ä‘Ã³:
- **Î» (lambda):** Eigenvalue (phÆ°Æ¡ng sai dá»c theo PC)
- **v:** Eigenvector (hÆ°á»›ng cá»§a PC)

#### **BÆ°á»›c 4: Chá»n k eigenvectors lá»›n nháº¥t**

Sáº¯p xáº¿p eigenvalues: Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»â‚™

Táº¡o ma tráº­n projection:
$W = [v_1, v_2, ..., v_k]$

#### **BÆ°á»›c 5: Transform dá»¯ liá»‡u**
$X_{new} = X \cdot W$

X_new cÃ³ k chiá»u thay vÃ¬ n chiá»u ban Ä‘áº§u.

#### **Explained Variance Ratio:**
$\text{EVR}_i = \frac{\lambda_i}{\sum_{j=1}^{n}\lambda_j}$

Tá»•ng EVR cá»§a k PCs Ä‘áº§u tiÃªn cho biáº¿t % thÃ´ng tin giá»¯ láº¡i.

---

### **3.2. t-SNE**

#### **BÆ°á»›c 1: TÃ­nh conditional probability trong khÃ´ng gian ban Ä‘áº§u**

XÃ¡c suáº¥t Ä‘iá»ƒm j lÃ  neighbor cá»§a i:
$p_{j|i} = \frac{\exp(-\lVert x_i - x_j \rVert^2 / 2\sigma_i^2)}{\sum_{k \neq i}\exp(-\lVert x_i - x_k \rVert^2 / 2\sigma_i^2)}$

Symmetric probability:
$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$

#### **BÆ°á»›c 2: TÃ­nh probability trong khÃ´ng gian tháº¥p chiá»u (2D/3D)**

DÃ¹ng t-distribution (heavy tail):
$q_{ij} = \frac{(1 + \lVert y_i - y_j \rVert^2)^{-1}}{\sum_{k \neq l}(1 + \lVert y_k - y_l \rVert^2)^{-1}}$

#### **BÆ°á»›c 3: Minimize Kullback-Leibler divergence**

$KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$

DÃ¹ng gradient descent Ä‘á»ƒ tá»‘i Æ°u vá»‹ trÃ­ cÃ¡c Ä‘iá»ƒm y trong khÃ´ng gian 2D/3D.

---

### **4. VÃ­ dá»¥ minh hoáº¡**

#### **4.1. PCA - Dataset 3D â†’ 2D**

**Dá»¯ liá»‡u ban Ä‘áº§u:** X cÃ³ 3 features (xâ‚, xâ‚‚, xâ‚ƒ), 100 samples.

**Sau chuáº©n hÃ³a vÃ  tÃ­nh eigenvalues:**
- Î»â‚ = 5.2 â†’ EVRâ‚ = 65%
- Î»â‚‚ = 2.1 â†’ EVRâ‚‚ = 26%
- Î»â‚ƒ = 0.7 â†’ EVRâ‚ƒ = 9%

**Chá»n k=2:** Giá»¯ láº¡i 91% thÃ´ng tin (65% + 26%).

**Káº¿t quáº£:** X_new cÃ³ 2 chiá»u (PC1, PC2).

#### **4.2. t-SNE - MNIST digits**

**Dataset:** 784 features (28Ã—28 pixels), 10 classes (digits 0-9).

**PCA vs t-SNE:**
- **PCA 2D:** CÃ¡c class chá»“ng lÃªn nhau, khÃ³ phÃ¢n biá»‡t
- **t-SNE 2D (perplexity=30):** 10 cá»¥m rÃµ rÃ ng tÆ°Æ¡ng á»©ng 10 chá»¯ sá»‘

**LÃ½ do:** t-SNE giá»¯ cáº¥u trÃºc non-linear tá»‘t hÆ¡n PCA.

---

### **5. Ghi chÃº vÃ  lÆ°u Ã½**

#### **5.1. So sÃ¡nh PCA vs t-SNE**

| TiÃªu chÃ­ | PCA | t-SNE |
|---------|-----|-------|
| **TÃ­nh cháº¥t** | Linear | Non-linear |
| **Má»¥c tiÃªu** | Maximize variance | Preserve local structure |
| **Tá»‘c Ä‘á»™** | Nhanh O(nÃ—dÂ²) | Cháº­m O(nÂ²) |
| **Interpretability** | PCs cÃ³ Ã½ nghÄ©a | KhÃ´ng interpret Ä‘Æ°á»£c |
| **Deterministic** | Yes | No (random init) |
| **Use case** | Preprocessing, feature extraction | Visualization |

#### **5.2. Khi nÃ o dÃ¹ng PCA?**
 Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc tuyáº¿n tÃ­nh
 Cáº§n giáº£m chiá»u nhanh (preprocessing cho ML)
 Muá»‘n giá»¯ láº¡i phÆ°Æ¡ng sai lá»›n nháº¥t
 Cáº§n interpret Ä‘Æ°á»£c cÃ¡c PC

#### **5.3. Khi nÃ o dÃ¹ng t-SNE?**
 Visualization dá»¯ liá»‡u cao chiá»u
 Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc phi tuyáº¿n phá»©c táº¡p
 Muá»‘n tháº¥y rÃµ clusters trong 2D/3D
 KhÃ´ng cáº§n interpret cÃ¡c trá»¥c

#### **5.4. LÆ°u Ã½ khi dÃ¹ng t-SNE**
- **Perplexity:** ThÆ°á»ng chá»n 5-50, dataset lá»›n dÃ¹ng 30-50
- **Learning rate:** ThÆ°á»ng 10-1000, Ä‘iá»u chá»‰nh náº¿u khÃ´ng há»™i tá»¥
- **Random:** Cháº¡y nhiá»u láº§n vá»›i seed khÃ¡c nhau
- **KhÃ´ng dÃ¹ng Ä‘á»ƒ transform data má»›i** (chá»‰ dÃ¹ng visualization)
- **Scale dá»¯ liá»‡u trÆ°á»›c:** Standardization quan trá»ng

#### **5.5. Biáº¿n thá»ƒ vÃ  má»Ÿ rá»™ng**
- **Incremental PCA:** Xá»­ lÃ½ dá»¯ liá»‡u lá»›n khÃ´ng vá»«a RAM
- **Kernel PCA:** PCA phi tuyáº¿n dÃ¹ng kernel trick
- **UMAP:** TÆ°Æ¡ng tá»± t-SNE nhÆ°ng nhanh hÆ¡n, preserve global structure
- **Autoencoders:** Deep learning approach cho giáº£m chiá»u

---

### **6. Key Takeaways**
 PCA: giáº£m chiá»u tuyáº¿n tÃ­nh, giá»¯ phÆ°Æ¡ng sai lá»›n nháº¥t  
 Chá»n k PCs dá»±a trÃªn Explained Variance Ratio (thÆ°á»ng â‰¥85%)  
 t-SNE: giáº£m chiá»u phi tuyáº¿n, giá»¯ cáº¥u trÃºc local  
 t-SNE chá»‰ dÃ¹ng visualization, khÃ´ng transform data má»›i  
 PCA nhanh vÃ  interpret Ä‘Æ°á»£c, t-SNE cháº­m nhÆ°ng visualization tá»‘t hÆ¡n  
 LuÃ´n chuáº©n hÃ³a dá»¯ liá»‡u trÆ°á»›c khi giáº£m chiá»u  

---

