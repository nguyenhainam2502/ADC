# IBM ‚Äì Unsupervised Machine Learning: Full Course Notes

## **MODULE 1: Introduction to Unsupervised Learning and K-Means**

### **1. T·ªïng quan n·ªôi dung**
Module ƒë·∫ßu ti√™n gi·ªõi thi·ªáu v·ªÅ **h·ªçc kh√¥ng gi√°m s√°t (Unsupervised Learning)** ‚Äì m·ªôt ph∆∞∆°ng ph√°p trong Machine Learning nh·∫±m **kh√°m ph√° c·∫•u tr√∫c ti·ªÅm ·∫©n trong d·ªØ li·ªáu kh√¥ng c√≥ nh√£n**. Thay v√¨ hu·∫•n luy·ªán t·ª´ c√°c c·∫∑p (x, y) nh∆∞ trong Supervised Learning, Unsupervised Learning t√¨m ki·∫øm **m·ªëi quan h·ªá gi·ªØa c√°c m·∫´u d·ªØ li·ªáu**.

Thu·∫≠t to√°n n·ªïi b·∫≠t trong ph·∫ßn n√†y l√† **K-Means Clustering**, m·ªôt ph∆∞∆°ng ph√°p ph√¢n c·ª•m (clustering) d·ª±a tr√™n kho·∫£ng c√°ch.

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Unsupervised Learning (H·ªçc kh√¥ng gi√°m s√°t):** Ph∆∞∆°ng ph√°p h·ªçc t·ª´ d·ªØ li·ªáu kh√¥ng c√≥ nh√£n, m·ª•c ti√™u l√† kh√°m ph√° c·∫•u tr√∫c ti·ªÅm ·∫©n.
- **Cluster (C·ª•m):** Nh√≥m c√°c ƒëi·ªÉm d·ªØ li·ªáu c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng cao.
- **Centroid (T√¢m c·ª•m):** Trung t√¢m c·ªßa m·ªôt c·ª•m, t√≠nh b·∫±ng trung b√¨nh c·ªông c√°c ƒëi·ªÉm trong c·ª•m.
- **K-Means Algorithm:** Thu·∫≠t to√°n ph√¢n c·ª•m ph·ªï bi·∫øn, l·∫∑p l·∫°i vi·ªác g√°n ƒëi·ªÉm d·ªØ li·ªáu v√†o c·ª•m g·∫ßn nh·∫•t v√† c·∫≠p nh·∫≠t t√¢m c·ª•m.
- **Iteration (L·∫∑p):** Qu√° tr√¨nh c·∫≠p nh·∫≠t li√™n t·ª•c t√¢m c·ª•m cho ƒë·∫øn khi h·ªôi t·ª•.

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**
M·ª•c ti√™u c·ªßa K-Means l√† **t·ªëi thi·ªÉu ho√° t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† t√¢m c·ª•m t∆∞∆°ng ·ª©ng**:

$$J = \sum_{i=1}^{k}\sum_{x \in C_i} \lVert x - \mu_i \rVert^2$$

Trong ƒë√≥:
- **J:** h√†m m·ª•c ti√™u (objective function) ‚Äì th·ªÉ hi·ªán ƒë·ªô sai l·ªách t·ªïng.
- **k:** s·ªë c·ª•m (clusters).
- **x:** ƒëi·ªÉm d·ªØ li·ªáu.
- **Œº·µ¢:** t√¢m c·ª•m th·ª© i (centroid of cluster i).
- **‚Äñx - Œº·µ¢‚Äñ¬≤:** b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch **Euclidean** gi·ªØa ƒëi·ªÉm x v√† t√¢m c·ª•m.

**Kho·∫£ng c√°ch Euclidean:**

$$d(x, \mu_i) = \sqrt{(x_1 - \mu_{i1})^2 + (x_2 - \mu_{i2})^2 + \dots + (x_n - \mu_{in})^2}$$

Kho·∫£ng c√°ch n√†y ƒëo ƒë·ªô kh√°c nhau gi·ªØa hai vector trong kh√¥ng gian n-chi·ªÅu.

---

### **4. V√≠ d·ª• minh ho·∫°**
Gi·∫£ s·ª≠ ta c√≥ 6 ƒëi·ªÉm d·ªØ li·ªáu tr√™n m·∫∑t ph·∫≥ng 2D, c·∫ßn chia th√†nh **k = 2** c·ª•m:

1. Ch·ªçn ng·∫´u nhi√™n 2 t√¢m c·ª•m ban ƒë·∫ßu.
2. G√°n m·ªói ƒëi·ªÉm v√†o c·ª•m c√≥ t√¢m g·∫ßn nh·∫•t.
3. C·∫≠p nh·∫≠t t√¢m c·ª•m b·∫±ng trung b√¨nh c·ªßa c√°c ƒëi·ªÉm trong c·ª•m.
4. L·∫∑p l·∫°i ƒë·∫øn khi t√¢m c·ª•m kh√¥ng thay ƒë·ªïi nhi·ªÅu.

**V√≠ d·ª• c·ª• th·ªÉ:**
- ƒêi·ªÉm: A(1,1), B(2,1), C(4,3), D(5,4), E(1,2), F(5,3)
- Kh·ªüi t·∫°o: Œº‚ÇÅ = (1,1), Œº‚ÇÇ = (5,4)
- Iteration 1: G√°n A,B,E v√†o C‚ÇÅ; C,D,F v√†o C‚ÇÇ
- C·∫≠p nh·∫≠t: Œº‚ÇÅ_new = (1.33, 1.33), Œº‚ÇÇ_new = (4.67, 3.33)
- L·∫∑p l·∫°i ƒë·∫øn khi h·ªôi t·ª•

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**
- C·∫ßn ch·ªçn tr∆∞·ªõc s·ªë c·ª•m k (th∆∞·ªùng x√°c ƒë·ªãnh b·∫±ng **Elbow Method**).
- D·ªØ li·ªáu n√™n ƒë∆∞·ª£c **chu·∫©n ho√° (Standardization)** ƒë·ªÉ tr√°nh bias do kh√°c thang ƒëo.
- K·∫øt qu·∫£ ph·ª• thu·ªôc v√†o **kh·ªüi t·∫°o t√¢m c·ª•m** (initialization); d√πng **K-Means++** ƒë·ªÉ t·ªëi ∆∞u ho√° ch·ªçn t√¢m ban ƒë·∫ßu.
- K-Means ho·∫°t ƒë·ªông t·ªët v·ªõi c·ª•m h√¨nh c·∫ßu, k√©m hi·ªáu qu·∫£ v·ªõi c·ª•m h√¨nh d·∫°ng ph·ª©c t·∫°p.

---

### **6. Key Takeaways**
 Unsupervised Learning kh√¥ng c·∫ßn nh√£n, t√¨m c·∫•u tr√∫c ·∫©n trong d·ªØ li·ªáu  
 K-Means t·ªëi ∆∞u h√≥a kho·∫£ng c√°ch ƒëi·ªÉm ƒë·∫øn t√¢m c·ª•m  
 C·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu v√† ch·ªçn k h·ª£p l√Ω (Elbow Method)  
 K-Means++ gi√∫p kh·ªüi t·∫°o t·ªët h∆°n, tr√°nh local minima  
 Ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ c·ª•m h√¨nh c·∫ßu v√† ph√¢n b·ªë ƒë·ªÅu  

---

## **MODULE 2: Distance Metrics & Computational Hurdles**

### **1. T·ªïng quan n·ªôi dung**
Module n√†y t·∫≠p trung v√†o **c√°c ƒë·ªô ƒëo kho·∫£ng c√°ch (Distance Metrics)** ‚Äì c√¥ng c·ª• c·ªët l√µi ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu trong clustering. Vi·ªác ch·ªçn metric ph√π h·ª£p ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m.

Ngo√†i ra, module ƒë·ªÅ c·∫≠p ƒë·∫øn **c√°c th√°ch th·ª©c t√≠nh to√°n** khi x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn v√† c√°ch gi·∫£i quy·∫øt.

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Euclidean Distance (Kho·∫£ng c√°ch Euclid):** Kho·∫£ng c√°ch ƒë∆∞·ªùng th·∫≥ng gi·ªØa hai ƒëi·ªÉm trong kh√¥ng gian.
- **Manhattan Distance (Kho·∫£ng c√°ch Manhattan):** T·ªïng ƒë·ªô ch√™nh l·ªách theo t·ª´ng tr·ª•c t·ªça ƒë·ªô (nh∆∞ ƒëi tr√™n l∆∞·ªõi √¥ vu√¥ng).
- **Minkowski Distance:** T·ªïng qu√°t h√≥a c·ªßa Euclidean v√† Manhattan v·ªõi tham s·ªë p.
- **Cosine Similarity (ƒê·ªô t∆∞∆°ng t·ª± Cosine):** ƒêo g√≥c gi·ªØa hai vector, ph√π h·ª£p v·ªõi d·ªØ li·ªáu vƒÉn b·∫£n.
- **Normalization/Standardization:** Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ c√πng thang ƒëo tr∆∞·ªõc khi t√≠nh kho·∫£ng c√°ch.
- **Computational Complexity (ƒê·ªô ph·ª©c t·∫°p t√≠nh to√°n):** Chi ph√≠ t√≠nh to√°n tƒÉng theo k√≠ch th∆∞·ªõc d·ªØ li·ªáu.

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**

#### **3.1. Euclidean Distance**
$$d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

ƒêo kho·∫£ng c√°ch ƒë∆∞·ªùng th·∫≥ng trong kh√¥ng gian n-chi·ªÅu. Nh·∫°y c·∫£m v·ªõi outliers v√† kh√°c bi·ªát v·ªÅ thang ƒëo.

#### **3.2. Manhattan Distance**
$$d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

T·ªïng gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa ch√™nh l·ªách tr√™n t·ª´ng chi·ªÅu. √çt b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers h∆°n Euclidean.

#### **3.3. Minkowski Distance**
$$d(x, y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$$

- **p = 1:** Manhattan Distance
- **p = 2:** Euclidean Distance
- **p ‚Üí ‚àû:** Chebyshev Distance (max|x·µ¢ - y·µ¢|)

#### **3.4. Cosine Similarity**
$$\text{cosine}(x,y) = \frac{x \cdot y}{\lVert x \rVert \lVert y \rVert} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}$$

**Cosine Distance:**
$$d_{\text{cosine}}(x,y) = 1 - \text{cosine}(x,y)$$

ƒêo ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªÅ h∆∞·ªõng, kh√¥ng quan t√¢m ƒë·ªô l·ªõn. Ph√π h·ª£p v·ªõi text mining, recommendation systems.

---

### **4. V√≠ d·ª• minh ho·∫°**

Cho hai ƒëi·ªÉm: **x = (1, 2)**, **y = (4, 6)**

#### **Euclidean:**
$$d = \sqrt{(1-4)^2 + (2-6)^2} = \sqrt{9+16} = \sqrt{25} = 5$$

#### **Manhattan:**
$$d = |1-4| + |2-6| = 3 + 4 = 7$$

#### **Cosine Similarity:**
$$\text{cosine} = \frac{1√ó4 + 2√ó6}{\sqrt{1^2+2^2} \cdot \sqrt{4^2+6^2}} = \frac{16}{\sqrt{5} \cdot \sqrt{52}} = \frac{16}{16.12} \approx 0.993$$

$$d_{\text{cosine}} = 1 - 0.993 = 0.007$$

**Nh·∫≠n x√©t:** Cosine distance r·∫•t nh·ªè ‚Üí hai vector g·∫ßn nh∆∞ c√πng h∆∞·ªõng.

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**

#### **5.1. Computational Hurdles (Th√°ch th·ª©c t√≠nh to√°n)**
- V·ªõi n ƒëi·ªÉm d·ªØ li·ªáu, t√≠nh t·∫•t c·∫£ kho·∫£ng c√°ch c·∫∑p: **O(n¬≤)** ‚Äì kh√¥ng kh·∫£ thi v·ªõi d·ªØ li·ªáu l·ªõn.
- K-Means chu·∫©n: m·ªói iteration t√≠nh kho·∫£ng c√°ch n√ók l·∫ßn ‚Üí **O(n√ók√ót√ód)** v·ªõi t iterations, d dimensions.

#### **5.2. Gi·∫£i ph√°p t·ªëi ∆∞u**
- **MiniBatch K-Means:** Ch·ªâ d√πng m·ªôt ph·∫ßn d·ªØ li·ªáu m·ªói iteration.
- **Approximate Nearest Neighbor (ANN):** D√πng c·∫•u tr√∫c d·ªØ li·ªáu nh∆∞ KD-Tree, Ball Tree.
- **Dimensionality Reduction:** D√πng PCA gi·∫£m s·ªë chi·ªÅu tr∆∞·ªõc khi clustering.
- **Parallel Computing:** T·∫≠n d·ª•ng GPU, distributed computing.

#### **5.3. L·ª±a ch·ªçn metric ph√π h·ª£p**
- **Euclidean:** D·ªØ li·ªáu s·ªë li·ªáu, c·ª•m h√¨nh c·∫ßu, ƒë√£ chu·∫©n h√≥a.
- **Manhattan:** D·ªØ li·ªáu c√≥ outliers, grid-based structures.
- **Cosine:** Text data, sparse vectors, quan t√¢m h∆∞·ªõng h∆°n ƒë·ªô l·ªõn.

---

### **6. Key Takeaways**
 Euclidean ph√π h·ª£p v·ªõi d·ªØ li·ªáu s·ªë ƒë√£ chu·∫©n h√≥a, c·ª•m compact  
 Manhattan √≠t nh·∫°y v·ªõi outliers, t·ªët cho d·ªØ li·ªáu grid  
 Cosine ƒëo ƒë·ªô t∆∞∆°ng ƒë·ªìng h∆∞·ªõng, l√Ω t∆∞·ªüng cho text/sparse data  
 Lu√¥n chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi t√≠nh kho·∫£ng c√°ch  
 V·ªõi d·ªØ li·ªáu l·ªõn: d√πng MiniBatch K-Means, ANN, ho·∫∑c PCA  

---

## **MODULE 3: Selecting a Clustering Algorithm**

### **1. T·ªïng quan n·ªôi dung**
Module n√†y h∆∞·ªõng d·∫´n c√°ch **l·ª±a ch·ªçn thu·∫≠t to√°n ph√¢n c·ª•m ph√π h·ª£p** d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu: k√≠ch th∆∞·ªõc, h√¨nh d·∫°ng c·ª•m, nhi·ªÖu, v√† m·ª•c ti√™u ph√¢n t√≠ch. M·ªói thu·∫≠t to√°n c√≥ ∆∞u nh∆∞·ª£c ƒëi·ªÉm ri√™ng, kh√¥ng c√≥ "best algorithm for all".

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Hierarchical Clustering (Ph√¢n c·ª•m ph√¢n c·∫•p):** X√¢y d·ª±ng c√¢y ph√¢n c·∫•p (dendrogram) th·ªÉ hi·ªán m·ªëi quan h·ªá gi·ªØa c√°c c·ª•m.
- **DBSCAN (Density-Based Spatial Clustering):** Ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô ƒëi·ªÉm.
- **Gaussian Mixture Model (GMM):** M√¥ h√¨nh h·ªón h·ª£p Gaussian, g√°n x√°c su·∫•t cho m·ªói ƒëi·ªÉm thu·ªôc c·ª•m.
- **Dendrogram:** Bi·ªÉu ƒë·ªì c√¢y th·ªÉ hi·ªán qu√° tr√¨nh g·ªôp/chia c·ª•m trong hierarchical clustering.
- **Soft Clustering:** Cho ph√©p m·ªôt ƒëi·ªÉm thu·ªôc nhi·ªÅu c·ª•m v·ªõi x√°c su·∫•t kh√°c nhau (GMM).
- **Hard Clustering:** M·ªói ƒëi·ªÉm ch·ªâ thu·ªôc ƒë√∫ng 1 c·ª•m (K-Means, DBSCAN).

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**

#### **3.1. Hierarchical Clustering**

**Agglomerative (Bottom-up):**
1. M·ªói ƒëi·ªÉm l√† m·ªôt c·ª•m
2. G·ªôp 2 c·ª•m g·∫ßn nh·∫•t
3. L·∫∑p l·∫°i ƒë·∫øn khi c√≤n 1 c·ª•m

**Linkage criteria:**
- **Single Linkage:** min d(a,b) v·ªõi a ‚àà C‚ÇÅ, b ‚àà C‚ÇÇ
- **Complete Linkage:** max d(a,b)
- **Average Linkage:** trung b√¨nh t·∫•t c·∫£ d(a,b)
- **Ward's Method:** t·ªëi thi·ªÉu variance khi g·ªôp c·ª•m

#### **3.2. Gaussian Mixture Model (GMM)**

$$P(x) = \sum_{i=1}^{k} \pi_i \cdot \mathcal{N}(x | \mu_i, \Sigma_i)$$

Trong ƒë√≥:
- **œÄ·µ¢:** Tr·ªçng s·ªë c·ª•m i (mixing coefficient), ‚àëœÄ·µ¢ = 1
- **ùí©(x|Œº·µ¢, Œ£·µ¢):** Ph√¢n ph·ªëi Gaussian v·ªõi mean Œº·µ¢ v√† covariance Œ£·µ¢
- **Œº·µ¢:** Vector trung b√¨nh c·ªßa c·ª•m i
- **Œ£·µ¢:** Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa c·ª•m i

**Ph√¢n ph·ªëi Gaussian ƒëa chi·ªÅu:**

$$\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$

**Thu·∫≠t to√°n EM (Expectation-Maximization):**
- **E-step:** T√≠nh x√°c su·∫•t ƒëi·ªÉm x thu·ªôc c·ª•m i
- **M-step:** C·∫≠p nh·∫≠t œÄ·µ¢, Œº·µ¢, Œ£·µ¢

---

### **4. V√≠ d·ª• minh ho·∫°**

#### **So s√°nh 4 thu·∫≠t to√°n tr√™n c√πng dataset:**

**Dataset:** D·ªØ li·ªáu 2D v·ªõi 3 c·ª•m kh√¥ng ƒë·ªÅu, c√≥ outliers.

| Thu·∫≠t to√°n | K·∫øt qu·∫£ |
|-----------|---------|
| **K-Means** | T·∫°o 3 c·ª•m h√¨nh c·∫ßu, outliers b·ªã g√°n sai |
| **Hierarchical** | Dendrogram cho th·∫•y c·∫•u tr√∫c ph√¢n c·∫•p, nh∆∞ng outliers v·∫´n trong c·ª•m |
| **DBSCAN** | Nh·∫≠n di·ªán ƒë√∫ng 3 c·ª•m + ƒë√°nh d·∫•u outliers l√† noise |
| **GMM** | T·∫°o 3 c·ª•m v·ªõi x√°c su·∫•t, m·ªôt s·ªë ƒëi·ªÉm bi√™n c√≥ x√°c su·∫•t trung gian |

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**

#### **5.1. B·∫£ng so s√°nh chi ti·∫øt**

| Ti√™u ch√≠ | K-Means | Hierarchical | DBSCAN | GMM |
|---------|---------|--------------|--------|-----|
| **S·ªë c·ª•m** | Ph·∫£i ch·ªçn tr∆∞·ªõc | C·∫Øt dendrogram | T·ª± ƒë·ªông | Ph·∫£i ch·ªçn tr∆∞·ªõc |
| **H√¨nh d·∫°ng c·ª•m** | H√¨nh c·∫ßu | Linh ho·∫°t | B·∫•t k·ª≥ | Ellipsoid |
| **Outliers** | Nh·∫°y c·∫£m | Nh·∫°y c·∫£m | X·ª≠ l√Ω t·ªët | Trung b√¨nh |
| **Complexity** | O(nkt) | O(n¬≤logn) | O(nlogn) | O(nk¬≤t) |
| **Soft/Hard** | Hard | Hard | Hard | Soft |
| **Scale** | T·ªët (n l·ªõn) | K√©m (n nh·ªè) | Trung b√¨nh | Trung b√¨nh |

#### **5.2. Decision Tree ƒë·ªÉ ch·ªçn thu·∫≠t to√°n**

```
D·ªØ li·ªáu c√≥ nhi·ªÖu/outliers?
‚îú‚îÄ YES ‚Üí DBSCAN
‚îî‚îÄ NO ‚Üí Bi·∫øt tr∆∞·ªõc s·ªë c·ª•m k?
    ‚îú‚îÄ YES ‚Üí C·ª•m h√¨nh c·∫ßu?
    ‚îÇ   ‚îú‚îÄ YES ‚Üí K-Means
    ‚îÇ   ‚îî‚îÄ NO ‚Üí GMM
    ‚îî‚îÄ NO ‚Üí Hierarchical
```

#### **5.3. L∆∞u √Ω khi √°p d·ª•ng**
- **K-Means:** Ch·∫°y nhi·ªÅu l·∫ßn v·ªõi kh·ªüi t·∫°o kh√°c nhau, d√πng K-Means++.
- **Hierarchical:** Ch·ªçn linkage method ph√π h·ª£p (Ward th∆∞·ªùng t·ªët nh·∫•t).
- **DBSCAN:** Tuning epsilon (Œµ) v√† MinPts quan tr·ªçng, d√πng k-distance graph.
- **GMM:** D·ªÖ overfit, c·∫ßn regularization ho·∫∑c BIC/AIC ƒë·ªÉ ch·ªçn k.

---

### **6. Key Takeaways**
 Kh√¥ng c√≥ thu·∫≠t to√°n t·ªët nh·∫•t cho m·ªçi tr∆∞·ªùng h·ª£p  
 K-Means: nhanh, ƒë∆°n gi·∫£n, nh∆∞ng gi·∫£ ƒë·ªãnh c·ª•m h√¨nh c·∫ßu  
 Hierarchical: kh√¥ng c·∫ßn ch·ªçn k, nh∆∞ng t·ªën b·ªô nh·ªõ  
 DBSCAN: x·ª≠ l√Ω outliers t·ªët, t√¨m c·ª•m h√¨nh d·∫°ng b·∫•t k·ª≥  
 GMM: soft clustering, linh ho·∫°t nh∆∞ng gi·∫£ ƒë·ªãnh Gaussian  
 Ch·ªçn thu·∫≠t to√°n d·ª±a tr√™n: h√¨nh d·∫°ng c·ª•m, outliers, quy m√¥ d·ªØ li·ªáu  

---

## **MODULE 4: Clustering Evaluation Metrics**

### **1. T·ªïng quan n·ªôi dung**
Sau khi ph√¢n c·ª•m, c·∫ßn **ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng** k·∫øt qu·∫£. Module n√†y gi·ªõi thi·ªáu c√°c metrics ƒë·ªÉ:
- ƒêo ƒë·ªô compact (ch·∫∑t ch·∫Ω) trong c·ª•m
- ƒêo ƒë·ªô separation (t√°ch bi·ªát) gi·ªØa c√°c c·ª•m
- So s√°nh c√°c thu·∫≠t to√°n/tham s·ªë kh√°c nhau

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Inertia/Within-Cluster Sum of Squares (WCSS):** T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch trong c·ª•m.
- **Silhouette Score:** ƒêo ƒë·ªô t√°ch bi·ªát gi·ªØa c√°c c·ª•m, t·ª´ -1 ƒë·∫øn 1.
- **Davies-Bouldin Index (DBI):** ƒêo t·ª∑ l·ªá gi·ªØa ph√¢n t√°n trong c·ª•m v√† kho·∫£ng c√°ch gi·ªØa c√°c c·ª•m.
- **Calinski-Harabasz Index:** T·ª∑ l·ªá gi·ªØa ph√¢n t√°n gi·ªØa c√°c c·ª•m v√† trong c·ª•m.
- **Elbow Method:** Ph∆∞∆°ng ph√°p ch·ªçn k b·∫±ng c√°ch t√¨m ƒëi·ªÉm g·∫•p kh√∫c tr√™n ƒë·ªì th·ªã Inertia.

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**

#### **3.1. Inertia (WCSS)**
$$J = \sum_{i=1}^{k}\sum_{x \in C_i} \lVert x - \mu_i \rVert^2$$

- C√†ng nh·ªè c√†ng t·ªët (c·ª•m c√†ng compact)
- Gi·∫£m khi k tƒÉng ‚Üí kh√¥ng d√πng ri√™ng ƒë·ªÉ ch·ªçn k
- D√πng Elbow Method ƒë·ªÉ t√¨m k t·ªëi ∆∞u

#### **3.2. Silhouette Score**

**Cho t·ª´ng ƒëi·ªÉm x:**
$$s(x) = \frac{b(x) - a(x)}{\max(a(x), b(x))}$$

Trong ƒë√≥:
- **a(x):** Kho·∫£ng c√°ch trung b√¨nh t·ª´ x ƒë·∫øn c√°c ƒëi·ªÉm kh√°c trong c√πng c·ª•m (cohesion)
- **b(x):** Kho·∫£ng c√°ch trung b√¨nh t·ª´ x ƒë·∫øn c√°c ƒëi·ªÉm trong c·ª•m g·∫ßn nh·∫•t kh√°c (separation)

**Silhouette Score t·ªïng th·ªÉ:**
$$S = \frac{1}{n}\sum_{i=1}^{n} s(x_i)$$

**Di·ªÖn gi·∫£i:**
- **s(x) ‚âà 1:** ƒêi·ªÉm ƒë∆∞·ª£c ph√¢n c·ª•m t·ªët
- **s(x) ‚âà 0:** ƒêi·ªÉm n·∫±m gi·ªØa 2 c·ª•m
- **s(x) < 0:** ƒêi·ªÉm c√≥ th·ªÉ b·ªã ph√¢n c·ª•m sai

#### **3.3. Davies-Bouldin Index (DBI)**
$$DBI = \frac{1}{k}\sum_{i=1}^{k} \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$$

Trong ƒë√≥:
- **œÉ·µ¢:** ƒê·ªô ph√¢n t√°n trung b√¨nh trong c·ª•m i
- **d(c·µ¢, c‚±º):** Kho·∫£ng c√°ch gi·ªØa t√¢m c·ª•m i v√† j

**Di·ªÖn gi·∫£i:** C√†ng nh·ªè c√†ng t·ªët (c·ª•m compact v√† t√°ch bi·ªát).

#### **3.4. Calinski-Harabasz Index (CH)**
$$CH = \frac{SS_B/(k-1)}{SS_W/(n-k)}$$

Trong ƒë√≥:
- **SS_B:** Between-cluster sum of squares
- **SS_W:** Within-cluster sum of squares
- **n:** S·ªë ƒëi·ªÉm, **k:** S·ªë c·ª•m

**Di·ªÖn gi·∫£i:** C√†ng l·ªõn c√†ng t·ªët (ph√¢n t√°n gi·ªØa c·ª•m l·ªõn, trong c·ª•m nh·ªè).

---

### **4. V√≠ d·ª• minh ho·∫°**

#### **4.1. Elbow Method**

Ch·∫°y K-Means v·ªõi k = 1, 2, 3, ..., 10, t√≠nh Inertia:

| k | Inertia |
|---|---------|
| 1 | 3500 |
| 2 | 1800 |
| 3 | 950 |
| 4 | 850 |
| 5 | 820 |

**ƒê·ªì th·ªã:** G·∫•p kh√∫c r√µ t·∫°i k=3 ‚Üí ch·ªçn k=3.

#### **4.2. Silhouette Score**

Dataset v·ªõi 100 ƒëi·ªÉm, ch·∫°y K-Means k=3:

- C·ª•m 1: 30 ƒëi·ªÉm, S_avg = 0.75
- C·ª•m 2: 45 ƒëi·ªÉm, S_avg = 0.82
- C·ª•m 3: 25 ƒëi·ªÉm, S_avg = 0.68

**T·ªïng:** S = (30√ó0.75 + 45√ó0.82 + 25√ó0.68)/100 = 0.77 ‚Üí T·ªët!

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**

#### **5.1. So s√°nh c√°c metrics**

| Metric | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | Gi√° tr·ªã t·ªët |
|--------|---------|------------|-------------|
| **Inertia** | ƒê∆°n gi·∫£n, nhanh | Kh√¥ng d√πng ri√™ng ƒë·ªÉ ch·ªçn k | C√†ng nh·ªè |
| **Silhouette** | Tr·ª±c quan, [-1,1] | Ch·∫≠m v·ªõi n l·ªõn | G·∫ßn 1 |
| **DBI** | X√©t c·∫£ cohesion & separation | Nh·∫°y v·ªõi outliers | C√†ng nh·ªè |
| **CH** | Hi·ªáu qu·∫£ v·ªõi n l·ªõn | Gi·∫£ ƒë·ªãnh c·ª•m convex | C√†ng l·ªõn |

#### **5.2. Quy tr√¨nh ƒë√°nh gi√°**

```python
# Pseudo-code
for k in range(2, 11):
    model = KMeans(n_clusters=k)
    labels = model.fit_predict(X)
    
    inertia[k] = model.inertia_
    silhouette[k] = silhouette_score(X, labels)
    dbi[k] = davies_bouldin_score(X, labels)
    ch[k] = calinski_harabasz_score(X, labels)

# V·∫Ω ƒë·ªì th·ªã v√† ch·ªçn k t·ªëi ∆∞u
```

#### **5.3. L∆∞u √Ω quan tr·ªçng**
- **Elbow Method:** ƒê√¥i khi kh√¥ng c√≥ ƒëi·ªÉm g·∫•p kh√∫c r√µ r√†ng.
- **Silhouette:** T√≠nh to√°n O(n¬≤) ‚Üí ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn.
- **K·∫øt h·ª£p nhi·ªÅu metrics:** Kh√¥ng n√™n ch·ªâ d·ª±a v√†o 1 ch·ªâ s·ªë.
- **Ground truth:** N·∫øu c√≥ nh√£n th·∫≠t, d√πng Adjusted Rand Index (ARI), Normalized Mutual Information (NMI).

---

### **6. Key Takeaways**
 Inertia gi·∫£m khi k tƒÉng, d√πng Elbow Method ƒë·ªÉ ch·ªçn k  
 Silhouette Score [-1,1]: g·∫ßn 1 l√† t·ªët, <0 l√† ph√¢n c·ª•m sai  
 DBI v√† CH ƒë√°nh gi√° c·∫£ cohesion v√† separation  
 N√™n k·∫øt h·ª£p nhi·ªÅu metrics ƒë·ªÉ ƒë√°nh gi√° to√†n di·ªán  
 V·ªõi ground truth: d√πng ARI, NMI thay v√¨ internal metrics  

---

## **MODULE 5: Density-Based Clustering and DBSCAN**

### **1. T·ªïng quan n·ªôi dung**
**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** l√† thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n **m·∫≠t ƒë·ªô ƒëi·ªÉm** thay v√¨ kho·∫£ng c√°ch ƒë·∫øn t√¢m c·ª•m. ∆Øu ƒëi·ªÉm l·ªõn: **nh·∫≠n di·ªán c·ª•m h√¨nh d·∫°ng b·∫•t k·ª≥** v√† **t·ª± ƒë·ªông ph√°t hi·ªán outliers**.

Kh√¥ng c·∫ßn ch·ªçn s·ªë c·ª•m k tr∆∞·ªõc, ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ nhi·ªÖu.

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Density (M·∫≠t ƒë·ªô):** S·ªë ƒëi·ªÉm trong m·ªôt v√πng l√¢n c·∫≠n.
- **Œµ (epsilon):** B√°n k√≠nh v√πng l√¢n c·∫≠n.
- **MinPts:** S·ªë ƒëi·ªÉm t·ªëi thi·ªÉu trong v√πng Œµ ƒë·ªÉ t·∫°o c·ª•m.
- **Core Point (ƒêi·ªÉm l√µi):** ƒêi·ªÉm c√≥ ‚â• MinPts ƒëi·ªÉm trong v√πng Œµ.
- **Border Point (ƒêi·ªÉm bi√™n):** N·∫±m trong v√πng Œµ c·ªßa core point nh∆∞ng kh√¥ng ƒë·ªß MinPts.
- **Noise Point (ƒêi·ªÉm nhi·ªÖu):** Kh√¥ng thu·ªôc c·ª•m n√†o.
- **Directly Density-Reachable:** ƒêi·ªÉm q trong v√πng Œµ c·ªßa core point p.
- **Density-Reachable:** C√≥ chu·ªói core points n·ªëi t·ª´ p ƒë·∫øn q.

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**

#### **3.1. ƒê·ªãnh nghƒ©a l√¢n c·∫≠n Œµ**

$N_{\varepsilon}(p) = \{q \in D \mid d(p,q) \leq \varepsilon\}$

T·∫≠p h·ª£p c√°c ƒëi·ªÉm q c√≥ kho·∫£ng c√°ch ƒë·∫øn p kh√¥ng qu√° Œµ.

#### **3.2. Core Point Condition**

ƒêi·ªÉm p l√† core point n·∫øu:
$|N_{\varepsilon}(p)| \geq MinPts$

#### **3.3. Directly Density-Reachable**

ƒêi·ªÉm q directly density-reachable t·ª´ p n·∫øu:
- p l√† core point
- q ‚àà N_Œµ(p)

#### **3.4. Density-Reachable**

q density-reachable t·ª´ p n·∫øu t·ªìn t·∫°i chu·ªói: p = p‚ÇÅ, p‚ÇÇ, ..., p‚Çô = q

Sao cho p·µ¢‚Çä‚ÇÅ directly density-reachable t·ª´ p·µ¢.

#### **3.5. Density-Connected**

p v√† q density-connected n·∫øu t·ªìn t·∫°i ƒëi·ªÉm o sao cho c·∫£ p v√† q ƒë·ªÅu density-reachable t·ª´ o.

**M·ªôt c·ª•m trong DBSCAN** l√† t·∫≠p t·∫•t c·∫£ c√°c ƒëi·ªÉm density-connected v·ªõi nhau.

---

### **4. V√≠ d·ª• minh ho·∫°**

#### **4.1. Thu·∫≠t to√°n DBSCAN t·ª´ng b∆∞·ªõc**

**Dataset:** 12 ƒëi·ªÉm, Œµ = 1.5, MinPts = 3

**B∆∞·ªõc 1:** Ch·ªçn ƒëi·ªÉm ch∆∞a thƒÉm (A)
- T√¨m N_Œµ(A) = {A, B, C, D} ‚Üí |N_Œµ(A)| = 4 ‚â• 3 ‚Üí A l√† core point
- T·∫°o Cluster 1 = {A, B, C, D}

**B∆∞·ªõc 2:** M·ªü r·ªông t·ª´ B (trong Cluster 1)
- N_Œµ(B) = {A, B, E} ‚Üí B l√† core point
- Th√™m E v√†o Cluster 1

**B∆∞·ªõc 3:** Ti·∫øp t·ª•c v·ªõi C, D, E...

**B∆∞·ªõc 4:** ƒêi·ªÉm X kh√¥ng ƒë·∫øn ƒë∆∞·ª£c t·ª´ core point n√†o ‚Üí X l√† noise

**K·∫øt qu·∫£:**
- Cluster 1: {A, B, C, D, E, F}
- Cluster 2: {G, H, I}
- Noise: {X, Y}

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**

#### **5.1. Ch·ªçn tham s·ªë Œµ v√† MinPts**

**MinPts:**
- Th∆∞·ªùng ch·ªçn: MinPts = 2√ódim (dim l√† s·ªë chi·ªÅu)
- T·ªëi thi·ªÉu: MinPts = 3 (cho d·ªØ li·ªáu 2D)
- D·ªØ li·ªáu nhi·ªÅu chi·ªÅu ho·∫∑c nhi·ªÖu: tƒÉng MinPts

**Œµ (epsilon):**
- D√πng **k-distance graph:**
  1. T√≠nh kho·∫£ng c√°ch ƒë·∫øn ƒëi·ªÉm th·ª© k g·∫ßn nh·∫•t (k = MinPts)
  2. S·∫Øp x·∫øp tƒÉng d·∫ßn
  3. V·∫Ω ƒë·ªì th·ªã, t√¨m ƒëi·ªÉm "g·∫•p kh√∫c" ‚Üí ch·ªçn Œµ

```python
# Pseudo-code k-distance
from sklearn.neighbors import NearestNeighbors
nbrs = NearestNeighbors(n_neighbors=MinPts).fit(X)
distances, indices = nbrs.kneighbors(X)
distances = np.sort(distances[:, -1])
plt.plot(distances)  # T√¨m elbow
```

#### **5.2. ∆Øu v√† Nh∆∞·ª£c ƒëi·ªÉm**

**∆Øu ƒëi·ªÉm:**
 Nh·∫≠n di·ªán c·ª•m h√¨nh d·∫°ng b·∫•t k·ª≥ (kh√¥ng ch·ªâ h√¨nh c·∫ßu)
 T·ª± ƒë·ªông ph√°t hi·ªán v√† lo·∫°i b·ªè outliers
 Kh√¥ng c·∫ßn ch·ªçn s·ªë c·ª•m k tr∆∞·ªõc
 Robust v·ªõi nhi·ªÖu

**Nh∆∞·ª£c ƒëi·ªÉm:**
 Nh·∫°y c·∫£m v·ªõi tham s·ªë Œµ v√† MinPts
 Kh√¥ng hi·ªáu qu·∫£ n·∫øu m·∫≠t ƒë·ªô c·ª•m thay ƒë·ªïi nhi·ªÅu
 Kh√≥ x·ª≠ l√Ω d·ªØ li·ªáu cao chi·ªÅu (curse of dimensionality)
 ƒê·ªô ph·ª©c t·∫°p O(n¬≤) ‚Üí ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn (c·∫£i thi·ªán b·∫±ng spatial index)

#### **5.3. Bi·∫øn th·ªÉ c·ªßa DBSCAN**

- **HDBSCAN:** Hierarchical DBSCAN, t·ª± ƒë·ªông ch·ªçn Œµ cho t·ª´ng v√πng
- **OPTICS:** Ordering Points To Identify Clustering Structure, kh√¥ng c·∫ßn ch·ªçn Œµ c·ªë ƒë·ªãnh
- **DBSCAN++:** T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v·ªõi spatial indexing

---

### **6. Key Takeaways**
 DBSCAN ph√¢n c·ª•m d·ª±a m·∫≠t ƒë·ªô, t√¨m c·ª•m h√¨nh d·∫°ng b·∫•t k·ª≥  
 T·ª± ƒë·ªông ph√°t hi·ªán outliers (noise points)  
 Kh√¥ng c·∫ßn ch·ªçn s·ªë c·ª•m k tr∆∞·ªõc  
 Tham s·ªë quan tr·ªçng: Œµ (b√°n k√≠nh) v√† MinPts (ng∆∞·ª°ng m·∫≠t ƒë·ªô)  
 D√πng k-distance graph ƒë·ªÉ ch·ªçn Œµ h·ª£p l√Ω  
 H·∫°n ch·∫ø: nh·∫°y v·ªõi tham s·ªë, k√©m hi·ªáu qu·∫£ n·∫øu m·∫≠t ƒë·ªô kh√¥ng ƒë·ªÅu  

---

## **MODULE 6: Dimensionality Reduction Techniques (PCA & t-SNE)**

### **1. T·ªïng quan n·ªôi dung**
**Gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction)** l√† qu√° tr√¨nh bi·∫øn ƒë·ªïi d·ªØ li·ªáu t·ª´ kh√¥ng gian nhi·ªÅu chi·ªÅu sang kh√¥ng gian √≠t chi·ªÅu h∆°n, trong khi v·∫´n **gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng**. M·ª•c ti√™u:
- Visualization (d·ªØ li·ªáu 2D/3D)
- Gi·∫£m chi ph√≠ t√≠nh to√°n
- Lo·∫°i b·ªè nhi·ªÖu v√† multicollinearity
- Ti·ªÅn x·ª≠ l√Ω tr∆∞·ªõc clustering

Hai k·ªπ thu·∫≠t ch√≠nh: **PCA (linear)** v√† **t-SNE (non-linear)**.

---

### **2. Thu·∫≠t ng·ªØ ch√≠nh (Key Terms)**
- **Principal Component Analysis (PCA):** T√¨m c√°c tr·ª•c ch√≠nh (principal components) gi·ªØ l·∫°i ph∆∞∆°ng sai l·ªõn nh·∫•t.
- **Eigenvalue (Tr·ªã ri√™ng):** ƒêo l∆∞·ª£ng ph∆∞∆°ng sai gi·∫£i th√≠ch b·ªüi m·ªói eigenvector.
- **Eigenvector (Vector ri√™ng):** H∆∞·ªõng c·ªßa principal component.
- **Covariance Matrix (Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai):** Ma tr·∫≠n ƒëo m·ªëi quan h·ªá tuy·∫øn t√≠nh gi·ªØa c√°c features.
- **Explained Variance Ratio:** T·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·ªØ l·∫°i b·ªüi t·ª´ng PC.
- **t-SNE (t-Distributed Stochastic Neighbor Embedding):** K·ªπ thu·∫≠t gi·∫£m chi·ªÅu phi tuy·∫øn, gi·ªØ c·∫•u tr√∫c c·ª•c b·ªô.
- **Perplexity:** Tham s·ªë t-SNE, c√¢n b·∫±ng gi·ªØa c·∫•u tr√∫c local v√† global.

---

### **3. C√¥ng th·ª©c to√°n h·ªçc & Gi·∫£i th√≠ch**

### **3.1. PCA (Principal Component Analysis)**

#### **B∆∞·ªõc 1: Chu·∫©n h√≥a d·ªØ li·ªáu**
$X_{std} = \frac{X - \mu}{\sigma}$

#### **B∆∞·ªõc 2: T√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai**
$\Sigma = \frac{1}{n-1}X^T X$

V·ªõi X ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a (mean = 0).

#### **B∆∞·ªõc 3: T√¨m eigenvalues v√† eigenvectors**

Gi·∫£i ph∆∞∆°ng tr√¨nh:
$\Sigma v = \lambda v$

Trong ƒë√≥:
- **Œª (lambda):** Eigenvalue (ph∆∞∆°ng sai d·ªçc theo PC)
- **v:** Eigenvector (h∆∞·ªõng c·ªßa PC)

#### **B∆∞·ªõc 4: Ch·ªçn k eigenvectors l·ªõn nh·∫•t**

S·∫Øp x·∫øp eigenvalues: Œª‚ÇÅ ‚â• Œª‚ÇÇ ‚â• ... ‚â• Œª‚Çô

T·∫°o ma tr·∫≠n projection:
$W = [v_1, v_2, ..., v_k]$

#### **B∆∞·ªõc 5: Transform d·ªØ li·ªáu**
$X_{new} = X \cdot W$

X_new c√≥ k chi·ªÅu thay v√¨ n chi·ªÅu ban ƒë·∫ßu.

#### **Explained Variance Ratio:**
$\text{EVR}_i = \frac{\lambda_i}{\sum_{j=1}^{n}\lambda_j}$

T·ªïng EVR c·ªßa k PCs ƒë·∫ßu ti√™n cho bi·∫øt % th√¥ng tin gi·ªØ l·∫°i.

---

### **3.2. t-SNE**

#### **B∆∞·ªõc 1: T√≠nh conditional probability trong kh√¥ng gian ban ƒë·∫ßu**

X√°c su·∫•t ƒëi·ªÉm j l√† neighbor c·ªßa i:
$p_{j|i} = \frac{\exp(-\lVert x_i - x_j \rVert^2 / 2\sigma_i^2)}{\sum_{k \neq i}\exp(-\lVert x_i - x_k \rVert^2 / 2\sigma_i^2)}$

Symmetric probability:
$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$

#### **B∆∞·ªõc 2: T√≠nh probability trong kh√¥ng gian th·∫•p chi·ªÅu (2D/3D)**

D√πng t-distribution (heavy tail):
$q_{ij} = \frac{(1 + \lVert y_i - y_j \rVert^2)^{-1}}{\sum_{k \neq l}(1 + \lVert y_k - y_l \rVert^2)^{-1}}$

#### **B∆∞·ªõc 3: Minimize Kullback-Leibler divergence**

$KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$

D√πng gradient descent ƒë·ªÉ t·ªëi ∆∞u v·ªã tr√≠ c√°c ƒëi·ªÉm y trong kh√¥ng gian 2D/3D.

---

### **4. V√≠ d·ª• minh ho·∫°**

#### **4.1. PCA - Dataset 3D ‚Üí 2D**

**D·ªØ li·ªáu ban ƒë·∫ßu:** X c√≥ 3 features (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ), 100 samples.

**Sau chu·∫©n h√≥a v√† t√≠nh eigenvalues:**
- Œª‚ÇÅ = 5.2 ‚Üí EVR‚ÇÅ = 65%
- Œª‚ÇÇ = 2.1 ‚Üí EVR‚ÇÇ = 26%
- Œª‚ÇÉ = 0.7 ‚Üí EVR‚ÇÉ = 9%

**Ch·ªçn k=2:** Gi·ªØ l·∫°i 91% th√¥ng tin (65% + 26%).

**K·∫øt qu·∫£:** X_new c√≥ 2 chi·ªÅu (PC1, PC2).

#### **4.2. t-SNE - MNIST digits**

**Dataset:** 784 features (28√ó28 pixels), 10 classes (digits 0-9).

**PCA vs t-SNE:**
- **PCA 2D:** C√°c class ch·ªìng l√™n nhau, kh√≥ ph√¢n bi·ªát
- **t-SNE 2D (perplexity=30):** 10 c·ª•m r√µ r√†ng t∆∞∆°ng ·ª©ng 10 ch·ªØ s·ªë

**L√Ω do:** t-SNE gi·ªØ c·∫•u tr√∫c non-linear t·ªët h∆°n PCA.

---

### **5. Ghi ch√∫ v√† l∆∞u √Ω**

#### **5.1. So s√°nh PCA vs t-SNE**

| Ti√™u ch√≠ | PCA | t-SNE |
|---------|-----|-------|
| **T√≠nh ch·∫•t** | Linear | Non-linear |
| **M·ª•c ti√™u** | Maximize variance | Preserve local structure |
| **T·ªëc ƒë·ªô** | Nhanh O(n√ód¬≤) | Ch·∫≠m O(n¬≤) |
| **Interpretability** | PCs c√≥ √Ω nghƒ©a | Kh√¥ng interpret ƒë∆∞·ª£c |
| **Deterministic** | Yes | No (random init) |
| **Use case** | Preprocessing, feature extraction | Visualization |

#### **5.2. Khi n√†o d√πng PCA?**
 D·ªØ li·ªáu c√≥ c·∫•u tr√∫c tuy·∫øn t√≠nh
 C·∫ßn gi·∫£m chi·ªÅu nhanh (preprocessing cho ML)
 Mu·ªën gi·ªØ l·∫°i ph∆∞∆°ng sai l·ªõn nh·∫•t
 C·∫ßn interpret ƒë∆∞·ª£c c√°c PC

#### **5.3. Khi n√†o d√πng t-SNE?**
 Visualization d·ªØ li·ªáu cao chi·ªÅu
 D·ªØ li·ªáu c√≥ c·∫•u tr√∫c phi tuy·∫øn ph·ª©c t·∫°p
 Mu·ªën th·∫•y r√µ clusters trong 2D/3D
 Kh√¥ng c·∫ßn interpret c√°c tr·ª•c

#### **5.4. L∆∞u √Ω khi d√πng t-SNE**
- **Perplexity:** Th∆∞·ªùng ch·ªçn 5-50, dataset l·ªõn d√πng 30-50
- **Learning rate:** Th∆∞·ªùng 10-1000, ƒëi·ªÅu ch·ªânh n·∫øu kh√¥ng h·ªôi t·ª•
- **Random:** Ch·∫°y nhi·ªÅu l·∫ßn v·ªõi seed kh√°c nhau
- **Kh√¥ng d√πng ƒë·ªÉ transform data m·ªõi** (ch·ªâ d√πng visualization)
- **Scale d·ªØ li·ªáu tr∆∞·ªõc:** Standardization quan tr·ªçng

#### **5.5. Bi·∫øn th·ªÉ v√† m·ªü r·ªông**
- **Incremental PCA:** X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn kh√¥ng v·ª´a RAM
- **Kernel PCA:** PCA phi tuy·∫øn d√πng kernel trick
- **UMAP:** T∆∞∆°ng t·ª± t-SNE nh∆∞ng nhanh h∆°n, preserve global structure
- **Autoencoders:** Deep learning approach cho gi·∫£m chi·ªÅu

---

### **6. Key Takeaways**
 PCA: gi·∫£m chi·ªÅu tuy·∫øn t√≠nh, gi·ªØ ph∆∞∆°ng sai l·ªõn nh·∫•t  
 Ch·ªçn k PCs d·ª±a tr√™n Explained Variance Ratio (th∆∞·ªùng ‚â•85%)  
 t-SNE: gi·∫£m chi·ªÅu phi tuy·∫øn, gi·ªØ c·∫•u tr√∫c local  
 t-SNE ch·ªâ d√πng visualization, kh√¥ng transform data m·ªõi  
 PCA nhanh v√† interpret ƒë∆∞·ª£c, t-SNE ch·∫≠m nh∆∞ng visualization t·ªët h∆°n  
 Lu√¥n chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi gi·∫£m chi·ªÅu  

---

